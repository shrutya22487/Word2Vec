{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordPieceTokenizer:\n",
    "    \n",
    "    \"\"\"\n",
    "    Initialisation: \n",
    "    1. Create a dictionary to store vocab and another to store freq\n",
    "    2. Define vocab size\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size : int):\n",
    "        \n",
    "        self.vocab = []\n",
    "        self.word_freq = {}\n",
    "        self.vocab_size = vocab_size\n",
    "    \n",
    "    \"\"\"\n",
    "    Preprocessing: \n",
    "    1. Convert to lower case\n",
    "    2. Split into words & remove spaces\n",
    "    3. Remove numbers & non-alphabetic chars like \",\", \".\", \"!\" etc. \n",
    "    4. Returns an array of strings\n",
    "    \"\"\"\n",
    "    def preprocess_data(self, text: str):\n",
    "        \n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text) #replacing characters using RegEx\n",
    "        return text.split()\n",
    "    \n",
    "    \"\"\"\n",
    "    Vocab Construction: \n",
    "    1. Preprocess the data and store in dict\n",
    "    \"\"\"\n",
    "    def construct_vocabulary(self, corpus: list):\n",
    "        \n",
    "        # store word frequencies\n",
    "        for sentence in corpus:\n",
    "            tokens = self.preprocess_data(sentence)\n",
    "            \n",
    "            for token in tokens:\n",
    "                self.word_freq[token] = self.word_freq.get(token, 0) + 1\n",
    "                \n",
    "        # create alphabet and splits\n",
    "        alphabet = []\n",
    "        splits = {}\n",
    "        for word in self.word_freq.keys():\n",
    "            if word[0] not in alphabet:\n",
    "                alphabet.append(word[0])\n",
    "            \n",
    "            for remaining_letter in word[1:]:\n",
    "                if f\"##{remaining_letter}\" not in alphabet:\n",
    "                    alphabet.append(f\"##{remaining_letter}\")\n",
    "\n",
    "            splits[word] = [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)]\n",
    "        alphabet.sort()\n",
    "        \n",
    "        self.vocab = [\"[PAD]\", \"[UNK]\"] + alphabet.copy()\n",
    "        \n",
    "        # compute scores for pairs\n",
    "        # merge pair\n",
    "        \"\"\"Deplag needed\"\"\"\n",
    "        while len(self.vocab) < self.vocab_size:\n",
    "            scores = self.compute_pair_scores(splits)\n",
    "            best_pair, max_score = \"\", None\n",
    "            for pair, score in scores.items():\n",
    "                if max_score is None or max_score < score:\n",
    "                    best_pair = pair\n",
    "                    max_score = score\n",
    "            splits = self.merge_pair(*best_pair, splits)\n",
    "            new_token = (\n",
    "                best_pair[0] + best_pair[1][2:]\n",
    "                if best_pair[1].startswith(\"##\")\n",
    "                else best_pair[0] + best_pair[1]\n",
    "            )\n",
    "            self.vocab.append(new_token)\n",
    "        \n",
    "        self.save_vocabulary()\n",
    "            \n",
    "    \"\"\"Deplag needed\"\"\"\n",
    "    def tokenize(self, text: str):\n",
    "        pre_tokenized_text = self.preprocess_data(text)\n",
    "        encoded_words = [self.encode_word(word) for word in pre_tokenized_text]\n",
    "        return sum(encoded_words, [])\n",
    "    \n",
    "    \n",
    "    #____________Helper Methods____________\n",
    "    \n",
    "    def save_vocabulary(self):\n",
    "        vocab_file = f\"vocabulary_86.txt\"\n",
    "        with open(vocab_file, \"w\") as f:\n",
    "            for token in self.vocab:\n",
    "                f.write(f\"{token}\\n\")\n",
    "    \n",
    "    \n",
    "    \"\"\"Deplag needed\"\"\"\n",
    "    def compute_pair_scores(self, splits):\n",
    "        letter_freqs = {}\n",
    "        pair_freqs = {}\n",
    "        for word, freq in self.word_freq.items():\n",
    "            split = splits[word]\n",
    "            if len(split) == 1:\n",
    "                letter_freqs[split[0]] = letter_freqs.get(split[0], 0) + freq\n",
    "                continue\n",
    "            for i in range(len(split) - 1):\n",
    "                pair = (split[i], split[i + 1]) \n",
    "                letter_freqs[split[i]] = letter_freqs.get(split[i], 0) + freq\n",
    "                pair_freqs[pair] = pair_freqs.get(pair, 0) + freq\n",
    "            letter_freqs[split[-1]] = letter_freqs.get(split[-1], 0) + freq\n",
    "\n",
    "        scores = {\n",
    "            pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n",
    "            for pair, freq in pair_freqs.items()\n",
    "        }\n",
    "        return scores\n",
    "    \n",
    "    \n",
    "    \"\"\"Deplag needed\"\"\"\n",
    "    def merge_pair(self, a, b, splits):\n",
    "        for word in self.word_freq:\n",
    "            split = splits[word]\n",
    "            if len(split) == 1:\n",
    "                continue\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == a and split[i + 1] == b:\n",
    "                    merge = a + b[2:] if b.startswith(\"##\") else a + b\n",
    "                    split = split[:i] + [merge] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            splits[word] = split\n",
    "        return splits\n",
    "    \n",
    "    \n",
    "    \"\"\"Deplag needed\"\"\"\n",
    "    def encode_word(self, word):\n",
    "        tokens = []\n",
    "        while len(word) > 0:\n",
    "            i = len(word)\n",
    "            while i > 0 and word[:i] not in self.vocab:\n",
    "                i -= 1\n",
    "            if i == 0:\n",
    "                return [\"[UNK]\"]\n",
    "            tokens.append(word[:i])\n",
    "            word = word[i:]\n",
    "            if len(word) > 0:\n",
    "                word = f\"##{word}\"\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "corpus = []\n",
    "with open(\"corpus.txt\", 'r') as file: \n",
    "    text = file.read()\n",
    "    sentences = text.split()\n",
    "    for sentence in sentences: \n",
    "        sentence = sentence.strip()\n",
    "        if sentence:\n",
    "            corpus.append(sentence)\n",
    "\n",
    "# print(corpus)\n",
    "tokenizer = WordPieceTokenizer(vocab_size=70)\n",
    "\n",
    "tokenizer.construct_vocabulary(corpus)\n",
    "vocab = tokenizer.vocab\n",
    "# print(tokenizer.vocab)\n",
    "\n",
    "\n",
    "input_json_path = 'sample_test.json'  # Replace with your input file path\n",
    "output_json_path = 'tokenized_data.json'  # Path to save tokenized output\n",
    "\n",
    "with open(input_json_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Prepare a dictionary to store the tokenized sentences\n",
    "tokenized_data = {}\n",
    "\n",
    "# Process each sentence in the input JSON\n",
    "for entry in data:\n",
    "    sentence_id = entry['id']\n",
    "    sentence = (entry['sentence'])  # Preprocess sentence\n",
    "    \n",
    "    # Tokenize the sentence\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "    # Store the tokens with the corresponding ID\n",
    "    tokenized_data[sentence_id] = tokens\n",
    "\n",
    "# Write the tokenized data to the output JSON file\n",
    "with open(output_json_path, 'w') as f:\n",
    "    json.dump(tokenized_data, f, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
